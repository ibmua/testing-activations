# Testing activations
Testing different activation functions in neural networks. This was my old code I wrote using Caffe. It seemed to prove that you can get much MUCH better results, actually, just awesome results, with 2 consecutive grouped layers with PReLU/ELU activations and can relatively easlily approximate functions vs what seems to be possible via 2 Maxout layers.

You'll likely need to change some paths to files. This is not intended for too broad an audience, if any. =)
