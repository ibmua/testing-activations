# Testing activations
Testing different activation functions in neural networks. This is my old code I wrote using Caffe. It seemed to prove that you can get much MUCH better results, actually, just awesome results, with 2 consecutive grouped layers with PReLU/ELU activations and can relatively easlily approximate functions vs what seems to be possible via 2 Maxout layers.
